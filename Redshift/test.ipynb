{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hellow\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import requests\n",
    "import json\n",
    "import csv\n",
    "import pandas as pd\n",
    "import redshift_connector\n",
    "import time\n",
    "import concurrent.futures\n",
    "import threading\n",
    "\n",
    "# Set Redshift variables\n",
    "host='default-workgroup.967842132715.us-east-2.redshift-serverless.amazonaws.com'\n",
    "port = 5439\n",
    "database = \"redshift_demo\"\n",
    "schema = \"public\"\n",
    "table = \"demo_table_large\"\n",
    "connection = \"redshift-connection-upass\"\n",
    "user='admin'\n",
    "password='Redshift1!'\n",
    "limit=2000000\n",
    "col_string = \"column\"\n",
    "\n",
    "# Set Spark variables\n",
    "url_batch = \"https://excel.test.coherent.global/coherent/api/v4/batch\"\n",
    "bearer_token = \"Bearer eyJhbGciOiJSUzI1NiIsInR5cCIgOiAiSldUIiwia2lkIiA6ICJaeHVzMXg5eXo3MXNNd3JyVWVqNldwcWducms5cmZGWERINHRVNDN0QVBrIn0.eyJleHAiOjE3MDc2MjYwNzksImlhdCI6MTcwNzYxODg3OSwiYXV0aF90aW1lIjoxNzA3NjE4ODc0LCJqdGkiOiI4NGQ1ZTFiZi01YWZkLTRmMmEtOTkyNi01NTQzOTg5NjdjNTQiLCJpc3MiOiJodHRwczovL2tleWNsb2FrLnRlc3QuY29oZXJlbnQuZ2xvYmFsL2F1dGgvcmVhbG1zL2NvaGVyZW50IiwiYXVkIjpbInByb2R1Y3QtZmFjdG9yeSIsInRlc3QtY2xpZW50IiwiYWNjb3VudCJdLCJzdWIiOiIzYTNlZDA3YS0zNGZhLTRkODktODFiOS04YzgxYTU0MzVlZTIiLCJ0eXAiOiJCZWFyZXIiLCJhenAiOiJwcm9kdWN0LWZhY3RvcnkiLCJub25jZSI6IjkyNWNiOWM0LWE2MDctNGY2NS05OTcyLWVjOWU3MDgxMTU0MiIsInNlc3Npb25fc3RhdGUiOiIzNTA2ODM3OS1lMzgyLTRhMWMtYjIzMy04YTA5NWFlM2FlZTkiLCJhY3IiOiIxIiwiYWxsb3dlZC1vcmlnaW5zIjpbImh0dHBzOi8vbW9kZWxpbmctY2VudGVyLmRldi5jb2hlcmVudC5nbG9iYWwiLCJodHRwczovL3NhLmRldi5jb2hlcmVudC5nbG9iYWwiLCJodHRwczovL2NvcGlsb3Quc3RhZ2luZy5jb2hlcmVudC5nbG9iYWwiLCJodHRwczovL2NvcGlsb3QuZGV2LmNvaGVyZW50Lmdsb2JhbCIsImh0dHBzOi8vc3BhcmstdXNlci1tYW5hZ2VyLnRlc3QuY29oZXJlbnQuZ2xvYmFsIiwiaHR0cHM6Ly9zYS5zdGFnaW5nLmNvaGVyZW50Lmdsb2JhbCIsImh0dHBzOi8vc3BhcmsudGVzdC5jb2hlcmVudC5nbG9iYWwiLCJodHRwOi8vbG9jYWxob3N0OjMwMDAiLCJodHRwczovL3NwYXJrLXVzZXItbWFuYWdlci50ZXN0LnllbGxvdy5jb2hlcmVudC5nbG9iYWwiLCJodHRwczovL21vZGVsaW5nLWNlbnRlci5zdGFnaW5nLmNvaGVyZW50Lmdsb2JhbCJdLCJyZWFsbV9hY2Nlc3MiOnsicm9sZXMiOlsiZGVmYXVsdC1yb2xlcy1jb2hlcmVudCIsIm9mZmxpbmVfYWNjZXNzIiwidW1hX2F1dGhvcml6YXRpb24iXX0sInJlc291cmNlX2FjY2VzcyI6eyJ0ZXN0LWNsaWVudCI6eyJyb2xlcyI6WyJ0ZXN0LXVzZXI6cGYiXX0sImFjY291bnQiOnsicm9sZXMiOlsibWFuYWdlLWFjY291bnQiLCJtYW5hZ2UtYWNjb3VudC1saW5rcyIsInZpZXctcHJvZmlsZSJdfX0sInNjb3BlIjoib3BlbmlkIGVtYWlsIHByb2ZpbGUiLCJzaWQiOiIzNTA2ODM3OS1lMzgyLTRhMWMtYjIzMy04YTA5NWFlM2FlZTkiLCJlbWFpbF92ZXJpZmllZCI6dHJ1ZSwibmFtZSI6Ik5pY2sgU3Rhbnppb25lIiwiZ3JvdXBzIjpbInVzZXI6cGYiXSwicmVhbG0iOiJjb2hlcmVudCIsInByZWZlcnJlZF91c2VybmFtZSI6Im5pY2suc3Rhbnppb25lQGNvaGVyZW50Lmdsb2JhbCIsImdpdmVuX25hbWUiOiJOaWNrIiwiZmFtaWx5X25hbWUiOiJTdGFuemlvbmUiLCJlbWFpbCI6Im5pY2suc3Rhbnppb25lQGNvaGVyZW50Lmdsb2JhbCJ9.qUfSvYs8fpsvuUBYktnF_E3WfD3KdSlXyVsuFHdBsdEYZc94LrtPJvR29pOnwbVcUWsMUcooeOEwl2TOzhRzjc_bWea66Lez1exEp0hcahIPeIbuxB1-qmdXvxh7I3cveLs2EE4QCCC533GK2OyxpHKWS5UVNXFpZoUcGN6ok6LPZDUZXRcOFUQdaZ_n4iQyCXHKk3b8XO4VVVHmwob3XlhgezRj3tlENuU2fFxBq-UVGwMfWpg7InJOLzaMUvpTfAk4HWmC_Huemdj-kqpOftdP-N1Mm9kAx0ylJh27S7RcT1V7Qn0-fJe80gRxt4ID2U8lVtsG0_T_LxZBhZow1Q\"\n",
    "spark_service = \"DemoStanz/basic_term_sample\"\n",
    "version_id = \"eeef4f7b-c191-46f7-831d-41b26291e58f\"\n",
    "headers_batch = {\n",
    "    'Content-Type': 'application/json',\n",
    "    'Authorization': bearer_token\n",
    "}\n",
    "\n",
    "payload_batch = {\n",
    "    'service_uri': spark_service,\n",
    "    'version_id': version_id,\n",
    "    'source_system': 'StanzRedshift', \n",
    "    'call_purpose': 'batch_api'\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_batch():\n",
    "    \"\"\"\n",
    "    Helper to both create the batch and send input data\n",
    "    Returns batch_id to be used in downstream functions\n",
    "    \n",
    "    \"\"\"\n",
    "    create_response = requests.post(url_batch, headers=headers_batch, json=payload_batch)\n",
    "    cr = create_response.json()\n",
    "    batch_id = cr['id']\n",
    "    \n",
    "    return batch_id\n",
    "\n",
    "\n",
    "def send_batch_inputs(batch_id, inputs):\n",
    "    \"\"\"\n",
    "    Helper to send input data\n",
    "    Returns records submitted to be used in downstream functions\n",
    "    \n",
    "    \"\"\"\n",
    "    input_url = f'{url_batch}/{batch_id}/data'\n",
    "    input_payload = {\n",
    "        'inputs': inputs\n",
    "    }\n",
    "    input_response = requests.post(input_url, headers=headers_batch, json=input_payload)\n",
    "    ir = input_response.json()\n",
    "    rs = ir['record_submitted']\n",
    "    \n",
    "    return rs, ir\n",
    "\n",
    "\n",
    "def get_batch_status(batch_id):\n",
    "    \"\"\"\n",
    "    Helper to grab the current status of the batch\n",
    "    \n",
    "    \"\"\"\n",
    "    status_url = f'{url_batch}/{batch_id}/status'\n",
    "    status_response = requests.get(status_url, headers=headers_batch)\n",
    "    sr = status_response.json()\n",
    "    \n",
    "    return sr\n",
    "\n",
    "\n",
    "def get_batch_results(batch_id):\n",
    "    \"\"\"\n",
    "    Helper to grab the outputs from the batch run\n",
    "    \n",
    "    \"\"\"\n",
    "    output_url = f'{url_batch}/{batch_id}/result'\n",
    "    output_response = requests.get(output_url, headers=headers_batch, params={'max':10000})\n",
    "    out = output_response.json()\n",
    "    count = out['count']\n",
    "    outputs = out['outputs']\n",
    "    \n",
    "    return count, outputs, out\n",
    "\n",
    "\n",
    "def close_batch(batch_id):\n",
    "    \"\"\"\n",
    "    Helper to close the batch\n",
    "    \n",
    "    \"\"\"\n",
    "    close_url = f'{url_batch}/{batch_id}/'\n",
    "    close = {\"batch_status\" : \"closed\"}\n",
    "    close_response = requests.patch(close_url, headers=headers_batch, json=close)\n",
    "    cr = close_response.json()\n",
    "    status = cr['batch_status']\n",
    "    \n",
    "    return status\n",
    "\n",
    "\n",
    "# Get JSON into V4 format\n",
    "def json_to_arr(js):\n",
    "    \"\"\"\n",
    "    Helper to transform JSON into V4 format\n",
    "    \n",
    "    \"\"\"\n",
    "    output = []\n",
    "    cols = list(js[0].keys())\n",
    "    output.append(cols)\n",
    "    for i in range(len(js)):\n",
    "        rec = []\n",
    "        dict = js[i]\n",
    "        keys = dict.keys()\n",
    "        for k in keys:\n",
    "            val = dict[k]\n",
    "            rec.append(val)\n",
    "        output.append(rec)\n",
    "    \n",
    "    return output\n",
    "\n",
    "def to_json(inputs, request_meta):\n",
    "    \"\"\"\n",
    "     Turn dataframe into JSON data structure\n",
    "    \n",
    "    \"\"\"\n",
    "    data_js = inputs.to_json(orient='records')\n",
    "    data_ls = eval(data_js)\n",
    "    #Create array of JSON requests\n",
    "    req = []\n",
    "    for i in range(len(inputs)):\n",
    "        request_data = {}\n",
    "        y = data_ls[i]\n",
    "        request_data['inputs'] = y\n",
    "        request = {\n",
    "            'request_data': request_data,\n",
    "            'request_meta': request_meta\n",
    "        }\n",
    "        req.append(request)\n",
    "    return req\n",
    "\n",
    "def pd_to_arr(df, flag=0):\n",
    "    \"\"\"\n",
    "    Helper to transform dataframe into V4 format\n",
    "    Specific function to inner.py\n",
    "\n",
    "    Note the specificity is around the position (last column) of the scenario in the dataframe\n",
    "    Future imporvement: Make the position of the json columns as an input array into the function \n",
    "    \n",
    "    \"\"\"\n",
    "    output = [] # Initialize array output\n",
    "    cols = df.columns.tolist() # Gather columns from dataframe\n",
    "    output.append(cols)\n",
    "    for i in range(len(df)): # Loop through records of dataframe\n",
    "        rec = df.iloc[i].tolist() # Convert current row elements into a list\n",
    "        if flag == -1: # Specifc for GenerateInners.py. Converts the first element in row into an embedded array.\n",
    "            yc = rec.pop(0) # Grab the first element of the list\n",
    "            js_yc = json.loads(yc)\n",
    "            y = json_to_arr(js_yc)\n",
    "            rec.insert(0, y)           \n",
    "        if flag == 1: # Specifc for inner.py. Converts the last element in row into an embedded array.\n",
    "            scen = rec.pop() # Grab the last element of the list\n",
    "            js = json.loads(scen)\n",
    "            x = json_to_arr(js)\n",
    "            rec.append(x)\n",
    "        output.append(rec)   \n",
    "    \n",
    "    return output\n",
    "\n",
    "\n",
    "def generate_batch(df):\n",
    "    df = df.astype({'ageentry': 'float', 'policyterm': 'float','sumassured': 'float'})\n",
    "    batch = pd_to_arr(df)\n",
    "    return batch\n",
    "\n",
    "def get_batch_results_with_lock(batch_id, api_lock):\n",
    "    with api_lock:\n",
    "        return get_batch_results(batch_id)\n",
    "\n",
    "def send_and_get_batch_results(batch_id, batch):\n",
    "    # Send batch inputs in a separate thread\n",
    "    with concurrent.futures.ThreadPoolExecutor() as executor:\n",
    "        send_future = executor.submit(send_batch_inputs, batch_id, batch)\n",
    "        records_submitted, input_response = send_future.result()  # Wait for the send_batch_inputs function to complete\n",
    "        print(input_response)\n",
    "    \n",
    "    # Get batch results in multiple threads\n",
    "    outputs = []\n",
    "    batch_records_gathered = 0\n",
    "    with concurrent.futures.ThreadPoolExecutor() as executor:\n",
    "        get_futures = [executor.submit(get_batch_results, batch_id) for _ in range(100)]\n",
    "\n",
    "        # Wait for all get_batch_results threads to complete\n",
    "        concurrent.futures.wait(get_futures)\n",
    "        \n",
    "        # Retrieve results\n",
    "        for future in get_futures:\n",
    "            try:\n",
    "                output = future.result()\n",
    "                outputs.append(output[1])\n",
    "                batch_records_gathered += output[0]\n",
    "            except Exception as e:\n",
    "                print(f\"Error in get_batch_results: {e}\")\n",
    "\n",
    "    return records_submitted, outputs, batch_records_gathered\n",
    "\n",
    "def threads_get_batch(batch_id):\n",
    "    # Get batch results in multiple threads\n",
    "    outputs = []\n",
    "    records_gathered = 0\n",
    "    api_lock = threading.Lock()\n",
    "    with concurrent.futures.ThreadPoolExecutor() as executor:\n",
    "        get_futures = [executor.submit(get_batch_results_with_lock, batch_id, api_lock) for _ in range(10)]\n",
    "\n",
    "        # Wait for all get_batch_results threads to complete\n",
    "        concurrent.futures.wait(get_futures)\n",
    "        \n",
    "        # Retrieve results\n",
    "        for future in get_futures:\n",
    "            try:\n",
    "                output = future.result()\n",
    "                outputs.append(output[1])\n",
    "                records_gathered += output[0]\n",
    "            except Exception as e:\n",
    "                print(f\"Error in get_batch_results: {e}\")\n",
    "\n",
    "    return outputs, records_gathered"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_batch(batch):\n",
    "\n",
    "    batch_id = create_batch()\n",
    "\n",
    "    # Intitialize variables for batch output loop\n",
    "    \n",
    "    records_submitted = 0\n",
    "    records_completed = 0\n",
    "    batch_records_gathered = 0\n",
    "\n",
    "    ## Not Threading: Test, loop every 10K\n",
    "    outputs = []\n",
    "    if len(batch) <= 500000:\n",
    "        records_submitted, input_response = send_batch_inputs(batch_id, batch)\n",
    "        print(input_response)\n",
    "    else:\n",
    "        cols = batch[0]\n",
    "        batch_size = 500000\n",
    "        for i in range(0, len(batch), batch_size):\n",
    "            print(i)\n",
    "            sub_batch = batch[i+1:i+1+batch_size]\n",
    "            sub_batch.insert(0, cols)\n",
    "            records_submitted, input_response = send_batch_inputs(batch_id, sub_batch)\n",
    "            print(input_response)\n",
    "\n",
    "            while batch_records_gathered < records_submitted:\n",
    "                try:\n",
    "                    records_gathered, output, out_response = get_batch_results(batch_id)\n",
    "                    output.pop(0)\n",
    "                    outputs.append(output)        \n",
    "                    batch_records_gathered += records_gathered\n",
    "                    print(f'Records Gathered: {batch_records_gathered}')\n",
    "\n",
    "                except KeyError or NameError:\n",
    "                    sr = get_batch_status(batch_id)\n",
    "                    print(f'Get Failed: {sr}')\n",
    "\n",
    "\n",
    "    sr = get_batch_status(batch_id)\n",
    "    rc = sr['records_completed']\n",
    "    records_completed = rc\n",
    "\n",
    "    # Check to ensure number of inputs = number processed = number retrieved\n",
    "    if records_submitted == records_completed and records_completed == batch_records_gathered:\n",
    "        print(\"Run Completed! Inputs = Outputs.\")\n",
    "    else:\n",
    "        print(\"Error I/O Mismatch\")\n",
    "        print(records_submitted, records_completed, batch_records_gathered)\n",
    "\n",
    "    # Close batch\n",
    "    status = close_batch(batch_id)\n",
    "    print(status)\n",
    "\n",
    "    return outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run_start = time.time()\n",
    "\n",
    "# Connects to Redshift cluster using AWS credentials\n",
    "start = time.time()\n",
    "conn = redshift_connector.connect(\n",
    "    host=host,\n",
    "    port=port,\n",
    "    database=database,\n",
    "    user='admin',\n",
    "    password='Redshift1!',\n",
    ")\n",
    "\n",
    "print(conn) \n",
    "cursor = conn.cursor()\n",
    "cursor.execute(f\"SELECT * FROM {database}.{schema}.{table} LIMIT {limit};\")\n",
    "result = cursor.fetchall()\n",
    "cursor.execute(f\"\"\"select \"column\" from pg_table_def where tablename = '{table}'\"\"\")\n",
    "columns = cursor.fetchall()\n",
    "\n",
    "end = time.time()\n",
    "print(f\"Get Redshift Data: {(end-start)}s\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "col_arr = [i[0] for i in list(columns)]\n",
    "df = pd.DataFrame(list(result), columns=col_arr)\n",
    "print(len(df))\n",
    "print(df.info())\n",
    "\n",
    "batch = generate_batch(df)\n",
    "print(batch[:10])\n",
    "print(len(batch))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
